## first i need to determine what things i need to setup and what keys to put into aws 
## then i need to think about the acutaly exectution of my analysis and find a true optimized plan with my current system

# üèóÔ∏è OSLIRA CLOUDFLARE WORKER - IMPLEMENTATION PLAN

**Status:** Production Architecture Finalized  
**Version:** 4.0 - Enterprise Grade  
**Timeline:** 10 weeks to production-ready foundation  
**Last Updated:** 2025-01-20

---

## üìä CURRENT vs TARGET STATE

### **Current Worker: 4/10**

**What's Wrong:**
- ‚ùå Everything synchronous (30s timeouts, no retry on failure)
- ‚ùå Service role bypasses RLS everywhere (security hole)
- ‚ùå No observability (can't track AI costs per user)
- ‚ùå No real-time progress (users refresh and lose status)
- ‚ùå Mixed concerns (40+ controller files, hard to navigate)
- ‚ùå No circuit breakers (external API failures cascade)

**What's Good:**
- ‚úÖ AWS Secrets integration works
- ‚úÖ Path aliases setup correct
- ‚úÖ Domain separation exists (api/domain/infrastructure/shared)

---

### **Target Worker: 95/100**

**What We're Building:**
- ‚úÖ Workflows = async orchestration (no 30s timeout)
- ‚úÖ Dual Supabase client (RLS enforced for reads, service role for system ops)
- ‚úÖ AI Gateway = cost savings + fallback + observability
- ‚úÖ Durable Objects = real-time WebSocket progress
- ‚úÖ Feature-first architecture (vertical slices, easy to find code)
- ‚úÖ Comprehensive testing (Vitest in Workers runtime)
- ‚úÖ Production observability (Analytics Engine + targeted Sentry)

**Why not 100/10?** Nothing is perfect until battle-tested in production.

---

## ‚úÖ FINALIZED - WILL IMPLEMENT

### **Phase 0: Foundation (Week 1)**
- ‚úÖ AWS Secrets Manager integration (DONE)
- ‚úÖ Config manager with caching (DONE)
- ‚úÖ Environment detection (production/staging) (DONE)
- üÜï Cloudflare bindings validation (KV, R2, Workflows, Queues, DO)
- üÜï Health check endpoints with binding status
- üÜï Apply RLS performance fix (wrap auth.uid() in subqueries)

### **Phase 1-2: Feature-First Architecture (Weeks 2-6)**

**Build vertical slices in order:**

#### **Credits Feature (Week 2)** - Simplest, validates pattern
```
features/credits/
‚îú‚îÄ‚îÄ domain/               # Credit business rules, calculator
‚îú‚îÄ‚îÄ application/          # Deduct/grant use cases
‚îú‚îÄ‚îÄ controllers/          # Balance/transactions endpoints
‚îî‚îÄ‚îÄ routes.ts
```

#### **Auth Feature (Week 3)** - Foundation for everything
```
features/auth/
‚îú‚îÄ‚îÄ domain/               # JWT validation rules
‚îú‚îÄ‚îÄ application/          # Login/verify use cases
‚îú‚îÄ‚îÄ controllers/          # Auth endpoints
‚îî‚îÄ‚îÄ routes.ts
```

#### **Business Profiles Feature (Week 4)** - User setup
```
features/business/
‚îú‚îÄ‚îÄ domain/               # Business rules, ICP logic
‚îú‚îÄ‚îÄ application/          # CRUD use cases
‚îú‚îÄ‚îÄ controllers/          # Business endpoints
‚îî‚îÄ‚îÄ routes.ts
```

#### **Analysis Feature (Weeks 5-6)** - Core product
```
features/analysis/
‚îú‚îÄ‚îÄ domain/               # Lead scoring, analysis rules
‚îú‚îÄ‚îÄ application/          # Analyze use cases
‚îú‚îÄ‚îÄ controllers/          # Analysis endpoints
‚îî‚îÄ‚îÄ routes.ts
```

### **Phase 3: Infrastructure Adapters (Ongoing)**

Built alongside features as needed:

```
infrastructure/
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ supabase.client.ts          # Dual client factory
‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.repository.ts      # Generic CRUD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.repository.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ credits.repository.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ business.repository.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ leads.repository.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis.repository.ts
‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ openai.adapter.ts
‚îÇ   ‚îú‚îÄ‚îÄ claude.adapter.ts
‚îÇ   ‚îî‚îÄ‚îÄ ai-gateway.client.ts        # FREE cost savings
‚îî‚îÄ‚îÄ scraping/
    ‚îî‚îÄ‚îÄ apify.adapter.ts
```

### **Phase 4: Shared Layer (Weeks 2-3)**

```
shared/
‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îú‚îÄ‚îÄ auth.middleware.ts          # JWT extraction
‚îÇ   ‚îú‚îÄ‚îÄ rate-limit.middleware.ts    # KV-based
‚îÇ   ‚îú‚îÄ‚îÄ error.middleware.ts         # Global handler
‚îÇ   ‚îî‚îÄ‚îÄ analytics.middleware.ts     # Track every request
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ logger.util.ts
‚îÇ   ‚îú‚îÄ‚îÄ validation.util.ts
‚îÇ   ‚îî‚îÄ‚îÄ response.util.ts
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ (feature-specific types stay in features/)
```

### **Phase 5: Async Orchestration (Weeks 7-8)**

```
core/workflows/
‚îú‚îÄ‚îÄ deep-analysis.workflow.ts       # 20min jobs, no timeout
‚îú‚îÄ‚îÄ bulk-analysis.workflow.ts
‚îî‚îÄ‚îÄ steps/
    ‚îú‚îÄ‚îÄ scrape-profile.step.ts      # Each step retries independently
    ‚îú‚îÄ‚îÄ run-ai-analysis.step.ts
    ‚îî‚îÄ‚îÄ save-results.step.ts

core/queues/
‚îú‚îÄ‚îÄ stripe-webhook.consumer.ts      # Fire-and-forget retries
‚îî‚îÄ‚îÄ email.consumer.ts

core/durable-objects/
‚îî‚îÄ‚îÄ analysis-progress.do.ts         # WebSocket real-time updates
```

### **Phase 6: Observability (Week 9)**

```
infrastructure/monitoring/
‚îú‚îÄ‚îÄ analytics-engine.client.ts      # FREE - log everything
‚îú‚îÄ‚îÄ sentry.client.ts                # PAID - critical errors only
‚îú‚îÄ‚îÄ error-classifier.ts             # INFO/WARN/ERROR/CRITICAL
‚îî‚îÄ‚îÄ retry-strategies.ts             # Critical/Standard/FastFail

shared/middleware/
‚îî‚îÄ‚îÄ analytics.middleware.ts         # Dimensions: user, cost, latency
```

### **Phase 7: Testing Infrastructure (Week 10)**

```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ credits/domain/*.test.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/application/*.test.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis/domain/*.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ       ‚îî‚îÄ‚îÄ database/*.test.ts
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ credits/balance-endpoint.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ analysis/analyze-endpoint.test.ts
‚îî‚îÄ‚îÄ e2e/
    ‚îî‚îÄ‚îÄ analysis-workflow.test.ts
```

**Testing Strategy:**
- ‚úÖ Vitest with `@cloudflare/vitest-pool-workers` (runs IN Workers runtime)
- ‚úÖ Domain layer: 100% coverage (pure business logic)
- ‚úÖ Application layer: 90% coverage (use cases)
- ‚úÖ Infrastructure: 70% coverage (mocked externals)
- ‚úÖ Controllers: 80% coverage (API endpoints)

---

## ‚ùå FINALIZED - WILL NOT IMPLEMENT

### **1. Gradual Deployments**
**Why:** Have staging environment. All-at-once deployment fine for team size. Adds complexity for minimal benefit.

### **2. D1 for Caching Layer**
**Why:** Supabase is source of truth. R2 + KV handle caching. Adding D1 = unnecessary layer.

### **3. Service Bindings for Microservices**
**Why:** Monolith works fine. Domain separation clear via features/. Only split if CPU limits hit (unlikely).

### **4. Workers AI (On-Platform Models)**
**Why:** OpenAI/Claude already contracted. Switching = quality risk for minimal savings.

### **5. Browser Rendering API**
**Why:** Not in feature roadmap. Add only when users request "export PDF".

### **6. AutoRAG / Vectorize**
**Why:** "Similar influencers" feature not in MVP. Add when building that capability.

### **7. Hyperdrive for Supabase**
**Why:** Supabase already sub-10ms. Not bottleneck. Test P95 latency first.

### **8. Supabase Database Webhooks**
**Why:** Workflows notify directly. No need for DB ‚Üí Webhook ‚Üí Worker callback.

---

## üîÆ DEFERRED - FUTURE ROADMAP

### **Phase 8: Apify Fallback (Post-MVP)**
**Status:** Marked for future implementation  
**Trigger:** If Apify downtime exceeds 1 hour/month OR costs >30% of budget

```
infrastructure/scraping/
‚îú‚îÄ‚îÄ apify.adapter.ts
‚îú‚îÄ‚îÄ bright-data.adapter.ts          # Fallback option 1
‚îî‚îÄ‚îÄ scraper-api.adapter.ts          # Fallback option 2
```

**Implementation:**
- AI Gateway-style routing
- Automatic failover on errors
- Cost tracking per provider

---

## üìã IMPLEMENTATION PHASES

### **Week 1: Phase 0 - Foundation**

**Deliverables:**
- ‚úÖ Validate all Cloudflare bindings (KV, R2, Workflows, Queues, DO)
- ‚úÖ Apply RLS performance fix (wrap all `auth.uid()` calls)
- ‚úÖ Health check endpoints
- ‚úÖ Verify SECURITY DEFINER on all RPC functions

**Success Criteria:**
- Worker boots without errors
- All bindings accessible
- Health check returns 200 with binding status
- Database queries 100x faster (RLS optimization)

---

### **Week 2: Phase 1 - Credits Feature**

**Deliverables:**
```
features/credits/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ credit-amount.vo.ts         # Value object (prevents negative)
‚îÇ   ‚îî‚îÄ‚îÄ credit-calculator.service.ts # Business rules
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îú‚îÄ‚îÄ get-balance.usecase.ts
‚îÇ   ‚îú‚îÄ‚îÄ get-transactions.usecase.ts
‚îÇ   ‚îî‚îÄ‚îÄ ports/
‚îÇ       ‚îî‚îÄ‚îÄ credit.repository.interface.ts
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îú‚îÄ‚îÄ balance.controller.ts       # GET /credits/balance
‚îÇ   ‚îî‚îÄ‚îÄ transactions.controller.ts  # GET /credits/transactions
‚îî‚îÄ‚îÄ routes.ts

infrastructure/database/repositories/
‚îî‚îÄ‚îÄ credits.repository.ts            # Implements interface

tests/unit/features/credits/
‚îú‚îÄ‚îÄ domain/credit-calculator.test.ts
‚îî‚îÄ‚îÄ application/get-balance.test.ts
```

**Success Criteria:**
- ‚úÖ Pattern validated (domain ‚Üí application ‚Üí infrastructure)
- ‚úÖ Endpoints return correct data
- ‚úÖ Tests pass (domain + application)
- ‚úÖ RLS enforced (user sees only their balance)

---

### **Week 3: Phase 2 - Auth Feature**

**Deliverables:**
```
features/auth/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îî‚îÄ‚îÄ jwt-validator.service.ts
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îî‚îÄ‚îÄ verify-token.usecase.ts
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îî‚îÄ‚îÄ auth.controller.ts
‚îî‚îÄ‚îÄ routes.ts

shared/middleware/
‚îî‚îÄ‚îÄ auth.middleware.ts              # Extract user from JWT
```

**Success Criteria:**
- ‚úÖ JWT validation works
- ‚úÖ Middleware extracts user ID
- ‚úÖ Unauthorized requests blocked

---

### **Week 4: Phase 3 - Business Profiles Feature**

**Deliverables:**
```
features/business/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ business-profile.entity.ts
‚îÇ   ‚îî‚îÄ‚îÄ icp-matcher.service.ts
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îú‚îÄ‚îÄ create-profile.usecase.ts
‚îÇ   ‚îú‚îÄ‚îÄ update-profile.usecase.ts
‚îÇ   ‚îî‚îÄ‚îÄ ports/
‚îÇ       ‚îî‚îÄ‚îÄ business.repository.interface.ts
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îî‚îÄ‚îÄ profiles.controller.ts
‚îî‚îÄ‚îÄ routes.ts

infrastructure/database/repositories/
‚îî‚îÄ‚îÄ business.repository.ts

infrastructure/
‚îî‚îÄ‚îÄ database/
    ‚îî‚îÄ‚îÄ supabase.client.ts          # ‚úÖ Dual client factory
```

**Success Criteria:**
- ‚úÖ Dual Supabase client pattern implemented
- ‚úÖ User client (anon key) for reads
- ‚úÖ Admin client (service role) for system operations
- ‚úÖ CRUD operations work
- ‚úÖ RLS enforced

---

### **Weeks 5-6: Phase 4 - Analysis Feature (Core Product)**

**Deliverables:**
```
features/analysis/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.entity.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lead.entity.ts
‚îÇ   ‚îú‚îÄ‚îÄ value-objects/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis-score.vo.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ instagram-handle.vo.ts
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îî‚îÄ‚îÄ lead-scorer.service.ts
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îú‚îÄ‚îÄ use-cases/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze-lead.usecase.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bulk-analyze.usecase.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anonymous-analyze.usecase.ts
‚îÇ   ‚îú‚îÄ‚îÄ ports/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai-provider.interface.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper.interface.ts
‚îÇ   ‚îî‚îÄ‚îÄ dtos/
‚îÇ       ‚îî‚îÄ‚îÄ analysis.dto.ts
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îú‚îÄ‚îÄ analyze.controller.ts
‚îÇ   ‚îú‚îÄ‚îÄ bulk-analyze.controller.ts
‚îÇ   ‚îî‚îÄ‚îÄ anonymous-analyze.controller.ts
‚îî‚îÄ‚îÄ routes.ts

infrastructure/
‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ openai.adapter.ts
‚îÇ   ‚îú‚îÄ‚îÄ claude.adapter.ts
‚îÇ   ‚îî‚îÄ‚îÄ ai-gateway.client.ts        # ‚úÖ Route through AI Gateway
‚îî‚îÄ‚îÄ scraping/
    ‚îî‚îÄ‚îÄ apify.adapter.ts
```

**Success Criteria:**
- ‚úÖ AI Gateway integrated (FREE caching + fallback)
- ‚úÖ Analysis creates lead, deducts credits, runs AI
- ‚úÖ Bulk analysis queues multiple jobs
- ‚úÖ Anonymous analysis rate-limited (KV)
- ‚úÖ All operations tested

---

### **Weeks 7-8: Phase 5 - Workflows + Durable Objects**

**Deliverables:**
```
core/workflows/
‚îú‚îÄ‚îÄ deep-analysis.workflow.ts
‚îú‚îÄ‚îÄ bulk-analysis.workflow.ts
‚îî‚îÄ‚îÄ steps/
    ‚îú‚îÄ‚îÄ scrape-profile.step.ts
    ‚îú‚îÄ‚îÄ run-ai-analysis.step.ts
    ‚îî‚îÄ‚îÄ save-results.step.ts

core/queues/
‚îî‚îÄ‚îÄ stripe-webhook.consumer.ts

core/durable-objects/
‚îî‚îÄ‚îÄ analysis-progress.do.ts

wrangler.toml
# Add bindings:
[[workflows]]
binding = "ANALYSIS_WORKFLOW"
class_name = "DeepAnalysisWorkflow"

[durable_objects]
bindings = [
  { name = "ANALYSIS_PROGRESS", class_name = "AnalysisProgressTracker" }
]

[[queues.producers]]
binding = "STRIPE_WEBHOOKS_QUEUE"
queue = "stripe-webhooks"
```

**Success Criteria:**
- ‚úÖ Workflows run 20min+ analyses without timeout
- ‚úÖ Durable Objects provide WebSocket real-time progress
- ‚úÖ Each workflow step retries independently
- ‚úÖ User can refresh browser, progress persists
- ‚úÖ Stripe webhooks retry via Queue if fail

---

### **Week 9: Phase 6 - Observability**

**Deliverables:**
```
infrastructure/monitoring/
‚îú‚îÄ‚îÄ analytics-engine.client.ts
‚îú‚îÄ‚îÄ sentry.client.ts
‚îú‚îÄ‚îÄ error-classifier.ts
‚îî‚îÄ‚îÄ retry-strategies.ts

shared/middleware/
‚îî‚îÄ‚îÄ analytics.middleware.ts
```

**Analytics Engine Integration:**
```typescript
// Every request tracked with dimensions:
{
  endpoint: '/v1/analyze',
  userId: 'user_123',
  accountId: 'acc_456',
  analysisType: 'deep',
  aiCost: 0.015,
  latencyMs: 450,
  statusCode: 200,
  creditsUsed: 2
}
```

**Success Criteria:**
- ‚úÖ Every request logged to Analytics Engine
- ‚úÖ Only CRITICAL errors go to Sentry
- ‚úÖ Can query: "Which users drive 80% of AI costs?"
- ‚úÖ Can query: "P95 latency per endpoint"
- ‚úÖ Retry strategies applied (Critical/Standard/FastFail)

---

### **Week 10: Phase 7 - Testing Infrastructure**

**Deliverables:**
```
vitest.config.ts
tests/
‚îú‚îÄ‚îÄ unit/
‚îú‚îÄ‚îÄ integration/
‚îî‚îÄ‚îÄ e2e/

package.json
"scripts": {
  "test": "vitest",
  "test:unit": "vitest run tests/unit",
  "test:integration": "vitest run tests/integration",
  "test:e2e": "vitest run tests/e2e"
}
```

**Success Criteria:**
- ‚úÖ Domain layer: 100% coverage
- ‚úÖ Application layer: 90% coverage
- ‚úÖ All tests run in Workers runtime (via vitest-pool-workers)
- ‚úÖ Integration tests use SELF binding
- ‚úÖ CI validates tests before deploy

---

## üéØ CRITICAL DECISIONS LOCKED IN

### **1. Dual Supabase Client Pattern**
**Status:** MANDATORY

```typescript
// infrastructure/database/supabase.client.ts
export class SupabaseClientFactory {
  // For reads - RLS enforced
  createUserClient(userId: string): SupabaseClient {
    return createClient(url, anonKey);
  }
  
  // For system operations - bypass RLS
  createAdminClient(): SupabaseClient {
    return createClient(url, serviceRoleKey);
  }
}
```

**Why:** Security-in-depth. Even if auth bugs exist, RLS protects data.

---

### **2. AI Gateway Integration**
**Status:** MANDATORY (FREE, saves money)

```typescript
// Before
const openai = new OpenAI({
  baseURL: "https://api.openai.com/v1"
});

// After (one-line change)
const openai = new OpenAI({
  baseURL: "https://gateway.ai.cloudflare.com/v1/{account}/{gateway}/openai"
});
```

**Benefits:**
- 30-40% cost reduction (caching duplicate prompts)
- Automatic fallback (OpenAI down ‚Üí Claude)
- Free analytics (track costs per user)
- Zero cost, zero downside

---

### **3. Workflow Error Handling**
**Status:** PHILOSOPHY LOCKED

**Only retry infrastructure failures, not business logic failures:**

```typescript
// ‚úÖ RETRY: Infrastructure failures
const profile = await step.do('scrape', async () => {
  return await scrapeProfile(username); // Retries if Apify times out
});

// ‚ùå NO RETRY: Business logic failures
const credits = await step.do('check_credits', async () => {
  if (userCredits < 2) {
    throw new BusinessLogicError('Insufficient credits');
  }
});
// Workflow stops immediately, no retry

// ‚úÖ RETRY: AI API failures
const analysis = await step.do('analyze', async () => {
  return await analyzeWithAI(profile); // Retries if OpenAI 503
});
```

---

### **4. Secret Rotation**
**Status:** Accept 5-minute cache delay

**Reasoning:** Secrets not rotating frequently. Simple > complex.

---

### **5. CI/CD**
**Status:** Manual deployment only

**Workflow:**
```bash
npm test                # Run Vitest suite
npm run typecheck       # Verify types
npm run deploy          # wrangler deploy (manual)
```

**No GitHub Actions.** You control deploys.

---

## üîß TECHNICAL SPECIFICATIONS

### **Folder Structure**

```
src/
‚îú‚îÄ‚îÄ features/           # Vertical slices
‚îÇ   ‚îú‚îÄ‚îÄ credits/
‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îú‚îÄ‚îÄ business/
‚îÇ   ‚îî‚îÄ‚îÄ analysis/
‚îú‚îÄ‚îÄ core/              # Horizontal services
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ scraping/
‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ queues/
‚îÇ   ‚îî‚îÄ‚îÄ durable-objects/
‚îú‚îÄ‚îÄ infrastructure/    # External world
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îî‚îÄ‚îÄ shared/           # Pure utilities
    ‚îú‚îÄ‚îÄ middleware/
    ‚îú‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ types/
```

---

### **Key Technologies**

| Layer | Technology | Version |
|-------|-----------|---------|
| Runtime | Cloudflare Workers | Latest |
| Framework | Hono | ^4.0.0 |
| Database | Supabase (Postgres) | Latest |
| Secrets | AWS Secrets Manager | - |
| AI Gateway | Cloudflare AI Gateway | FREE |
| Workflows | Cloudflare Workflows | - |
| Queues | Cloudflare Queues | - |
| Real-time | Durable Objects | - |
| Testing | Vitest + @cloudflare/vitest-pool-workers | 2.0.x |
| Observability | Workers Analytics Engine (FREE) | - |
| Error Tracking | Sentry (critical only) | - |

---

### **Architecture Principles**

1. **Feature-first:** Each feature owns its domain/application/controllers
2. **Ports & Adapters:** Domain defines interfaces, infrastructure implements
3. **Dependency Rule:** Outer layers depend on inner, never reverse
4. **Single Responsibility:** Each module does ONE thing well
5. **Fail-safe Defaults:** Default deny, explicit grant
6. **Observable:** Every request tracked, every error classified

---

## üìä SUCCESS METRICS

### **Performance**

| Metric | Current | Target |
|--------|---------|--------|
| P95 Analysis Latency | 30s (timeout) | 20s (async) |
| Credit Check Latency | 500ms | <50ms |
| RLS Query Performance | Slow | 100x faster |
| Workflow Success Rate | N/A | >99% |

### **Cost**

| Item | Current | With Optimization |
|------|---------|-------------------|
| AI Calls/Month | $300 | $210 (30% cache hit) |
| Worker CPU | $100 | $2 (Workflows) |
| Total | $400 | $212 |
| **Savings** | - | **$188/month** |

### **Reliability**

| Metric | Current | Target |
|--------|---------|--------|
| Apify Downtime Impact | 100% (single point of failure) | 0% (future fallback) |
| Stripe Webhook Loss | Possible | 0% (Queue retries) |
| Analysis Job Failure | Lost | Auto-retry per step |

---

## üöÄ DEPLOYMENT PLAN

### **Week 1: Foundation**
```bash
git checkout -b phase-0-foundation
# Apply RLS fixes, validate bindings
npm run deploy
# Test on staging
```

### **Weeks 2-6: Features (Incremental)**
```bash
git checkout -b feature/credits
# Build credits feature
npm test
npm run deploy
# Validate in production

git checkout -b feature/auth
# Build auth feature
# ... repeat for each feature
```

### **Weeks 7-8: Workflows**
```bash
git checkout -b phase-5-workflows
# Add Workflows + DO + Queues
# Update wrangler.toml
npm run deploy
# Monitor for 1 week
```

### **Weeks 9-10: Observability + Testing**
```bash
git checkout -b phase-6-observability
# Add Analytics Engine + Sentry
npm test  # Full test suite
npm run deploy
```

---

## üìà POST-IMPLEMENTATION

### **Week 11: Production Monitoring**

**Watch for:**
- Analytics Engine dashboard (track AI costs, latency, errors)
- Sentry alerts (should be rare - only critical)
- Workflow success rate (should be >99%)
- Cache hit rate (should increase over time)

### **Week 12: Performance Tuning**

**Optimize based on real data:**
- Which endpoints are slowest? (Analytics Engine)
- Which AI calls cost most? (AI Gateway logs)
- Which workflows fail most? (Workflow introspection)

### **Week 13+: Feature Development**

**With solid foundation, build:**
- New analysis types
- Enhanced business context
- Advanced filtering
- Bulk operations
- Admin dashboards

---

## üéØ FINAL RATING

### **Current Worker: 4/10**
- Synchronous blocking
- Security holes
- No observability
- Poor structure

### **Target Worker: 95/100**
- Async orchestration
- Defense-in-depth security
- Full observability
- Clean architecture
- Comprehensive testing
- Production-ready

### **Why not 100?**
Nothing is perfect until proven in production. This is a **95/100 foundation** that will evolve to 100/100 through real-world usage.

---

## ‚úÖ READY TO BUILD

**All decisions finalized. All phases defined. All technologies selected.**

**Start with Phase 0 - Week 1 begins now.**

---

**END OF IMPLEMENTATION PLAN**

# üéØ FINALIZED OPTIMIZATION PLAN - COMPLETE REFERENCE

Based on our entire conversation, here's everything we agreed upon, organized by priority.

---

## **üìã EXECUTIVE SUMMARY**

### **Current Performance:**
- **LIGHT:** 10-13s (8s Apify + 2-3s AI)
- **DEEP:** 30s (8s Apify + 20s AI sequential)
- **XRAY:** 35s (8s Apify + 25s AI mixed)

### **Optimized Targets:**
- **LIGHT:** 8-11s cold, 2-3s cached (avg ~6s with 50% cache)
- **DEEP:** 18-23s cold, 12-15s cached
- **XRAY:** 16-18s cold, 10-12s cached

### **Key Constraints:**
- Apify: 6-8s minimum (unavoidable bottleneck)
- Apify concurrency: 5-10 runs (Starter plan limit)
- Bulk 100 profiles: 60-120 seconds (batched, not parallel)

---

## **üéØ PHASE 1: CRITICAL OPTIMIZATIONS (Week 1-2)**

### **1. GLOBAL PROFILE CACHING**

**Why:** 50%+ of analyses will hit cache after momentum builds, making them instant.

**Implementation:**
```typescript
// features/analysis/infrastructure/cache/r2-cache.service.ts

export class R2CacheService {
  private bucket: R2Bucket;
  
  async get(username: string, analysisType: AnalysisType): Promise<CachedProfile | null> {
    const key = `instagram:${username}:v1`;
    const cached = await this.bucket.get(key);
    
    if (!cached) return null;
    
    const data = await cached.json() as CachedProfile;
    const age = Date.now() - new Date(data.cached_at).getTime();
    
    // Different TTLs by analysis type
    const ttls = {
      light: 24 * 60 * 60 * 1000,  // 24 hours
      deep: 12 * 60 * 60 * 1000,   // 12 hours
      xray: 6 * 60 * 60 * 1000     // 6 hours
    };
    
    if (age > ttls[analysisType]) {
      return null; // Expired
    }
    
    return data;
  }
  
  async set(username: string, profileData: ProfileData): Promise<void> {
    const key = `instagram:${username}:v1`;
    const cacheData = {
      profile: profileData,
      cached_at: new Date().toISOString(),
      username: profileData.username,
      followers: profileData.followersCount
    };
    
    await this.bucket.put(key, JSON.stringify(cacheData));
  }
}
```

**Impact:**
- 50% of requests skip Apify entirely after Month 1
- Cache hits: <1s (vs 8s scraping)
- Cost savings: $0.01 per cached request

---

### **2. PROMPT CACHING (Business Context)**

**Why:** Business context repeated across 100s of analyses = wasted cost + speed.

**Implementation:**
```typescript
// features/analysis/domain/services/ai-adapter.service.ts

export async function executeWithPromptCache(
  profile: ProfileData,
  business: BusinessProfile,
  analysisType: AnalysisType
) {
  // Mark business context for caching
  const messages = [
    {
      role: 'system',
      content: buildSystemPrompt(analysisType)
    },
    {
      role: 'user',
      content: [
        // CACHED SECTION (business context)
        {
          type: 'text',
          text: buildBusinessContext(business),
          cache_control: { type: 'ephemeral' } // Claude caching
        },
        // DYNAMIC SECTION (profile data - NOT cached)
        {
          type: 'text',
          text: buildProfileData(profile)
        }
      ]
    }
  ];
  
  // For OpenAI: Place business context first (auto-cached after 1024 tokens)
  // For Claude: Use cache_control marker
  
  return await openai.chat.completions.create({
    model: 'gpt-4o',
    messages
  });
}
```

**Impact:**
- Business context (800 tokens): 90% cheaper after first use
- Speed: 2-4s faster per analysis (cached tokens process faster)
- Cost: $0.003 ‚Üí $0.0003 per analysis

---

### **3. FULL AI PARALLELIZATION**

**Why:** Current sequential AI calls waste 10-15 seconds.

**Current (Bad):**
```typescript
// DEEP: Sequential
const core = await executeCoreStrategy();      // 15s
const [outreach, personality] = await Promise.all([
  executeOutreach(),   // 4s
  executePersonality() // 4s
]); // Total: 15s + 4s = 19s
```

**Optimized (Good):**
```typescript
// DEEP: Full parallel
const [core, outreach, personality] = await Promise.all([
  executeCoreStrategy(),   // 15s
  executeOutreach(),       // 4s
  executePersonality()     // 4s
]);
// Total: max(15s, 4s, 4s) = 15s (saved 4s)

// XRAY: Full parallel
const [psycho, commercial, outreach, personality] = await Promise.all([
  executePsychographic(),  // 10s
  executeCommercial(),     // 6s
  executeOutreach(),       // 4s
  executePersonality()     // 4s
]);
// Total: max(10s, 6s, 4s, 4s) = 10s (saved 12s)
```

**Impact:**
- DEEP: 19s ‚Üí 15s (4s faster)
- XRAY: 22s ‚Üí 10s (12s faster)

---

### **4. ASYNC EXECUTION (Queue + Workflows)**

**Why:** User shouldn't wait 20s staring at loading spinner.

**Implementation:**
```typescript
// features/analysis/application/use-cases/analyze.usecase.ts

export async function handleAnalyzeRequest(
  username: string,
  analysisType: AnalysisType,
  businessId: string,
  userId: string
) {
  // Generate run_id immediately
  const run_id = generateRunId();
  
  // Create pending record
  await db.insert('runs', {
    run_id,
    user_id: userId,
    business_id: businessId,
    status: 'pending',
    created_at: new Date()
  });
  
  // Queue for background processing
  await queue.send({
    type: 'analyze',
    run_id,
    username,
    analysisType,
    businessId,
    userId
  });
  
  // Return immediately (400ms response)
  return {
    run_id,
    status: 'queued',
    estimated_time: getEstimatedTime(analysisType),
    poll_url: `/runs/${run_id}`
  };
}

// Background worker
export async function processAnalysis(message: QueueMessage) {
  const { run_id, username, analysisType, businessId, userId } = message;
  
  try {
    // Update status
    await updateRunStatus(run_id, 'analyzing');
    
    // Check cache
    const cached = await cache.get(username, analysisType);
    if (cached) {
      await updateRunStatus(run_id, 'cache_hit');
      return await completeFromCache(run_id, cached);
    }
    
    // Scrape + analyze
    const profile = await scrapeProfile(username, analysisType);
    const analysis = await analyzeProfile(profile, business, analysisType);
    
    // Save results
    await saveAnalysis(run_id, analysis);
    await updateRunStatus(run_id, 'complete');
    
  } catch (error) {
    await updateRunStatus(run_id, 'failed', error.message);
  }
}
```

**User Experience:**
```
POST /v1/analyze
  ‚Üì (400ms)
Response:
{
  run_id: 'run_abc123',
  status: 'queued',
  estimated_time: '18-23 seconds',
  poll_url: '/runs/run_abc123'
}

// Frontend polls every 2s
GET /runs/run_abc123
{
  status: 'analyzing',
  progress: 'Scraping profile...',
  elapsed: 8
}

GET /runs/run_abc123
{
  status: 'complete',
  result: { score: 85, ... },
  processing_time: 19
}
```

---

### **5. INTELLIGENT MODEL SELECTION**

**Why:** gpt-5-nano fails on complex tasks, wastes time reasoning.

**Current Issues:**
- LIGHT uses gpt-5-mini (should use gpt-4o-mini - faster for simple tasks)
- Reasoning effort not optimized per task
- No model fallback strategy

**Optimized Config:**
```typescript
// features/analysis/domain/config/ai-models.config.ts

export const AI_MODEL_CONFIG = {
  LIGHT: {
    tasks: {
      scoring: {
        model: 'gpt-4o-mini',        // Fast, cheap, good at JSON
        reasoning_effort: 'low',
        max_tokens: 1000,
        temperature: 0,
        expected_time: '2-3s',
        fallback: 'gpt-5-mini'
      }
    }
  },
  
  DEEP: {
    tasks: {
      core_strategy: {
        model: 'gpt-5-mini',         // Smart enough, not overkill
        reasoning_effort: 'medium',
        max_tokens: 3000,
        temperature: 0.4,
        expected_time: '12-15s',
        is_bottleneck: true
      },
      outreach: {
        model: 'gpt-5-mini',
        reasoning_effort: 'low',
        max_tokens: 2000,
        temperature: 0.6,
        expected_time: '3-4s'
      },
      personality: {
        model: 'gpt-5-mini',
        reasoning_effort: 'low',
        max_tokens: 1500,
        temperature: 0.3,
        expected_time: '3-4s'
      }
    }
  },
  
  XRAY: {
    tasks: {
      psychographic: {
        model: 'gpt-5',              // Premium model for deep insight
        reasoning_effort: 'medium',
        max_tokens: 4000,
        temperature: 0.5,
        expected_time: '7-10s',
        justification: 'Requires deepest psychological insight'
      },
      commercial: {
        model: 'gpt-5-mini',
        reasoning_effort: 'medium',
        max_tokens: 3000,
        temperature: 0.4,
        expected_time: '5-6s'
      },
      outreach: {
        model: 'gpt-5-mini',
        reasoning_effort: 'low',
        max_tokens: 2000,
        temperature: 0.6,
        expected_time: '3-4s'
      },
      personality: {
        model: 'gpt-5-mini',
        reasoning_effort: 'low',
        max_tokens: 1500,
        temperature: 0.3,
        expected_time: '3-4s'
      }
    }
  }
};
```

---

## **üéØ PHASE 2: INFRASTRUCTURE IMPROVEMENTS (Week 3-4)**

### **6. COMPREHENSIVE COST TRACKING**

**Current Problem:** Only tracking AI costs (~40% of total).

**Full Cost Tracking:**
```typescript
// features/analysis/domain/services/cost-tracker.service.ts

export class CostTracker {
  private costs = {
    apify: 0,
    ai_calls: [] as AICallCost[],
    total: 0
  };
  
  trackApifyCall(postsScraped: number, duration: number) {
    // Apify charges ~$0.001 per 100 posts or $0.01 per 10s of compute
    const cost = Math.max(
      (postsScraped / 100) * 0.001,
      (duration / 10000) * 0.01
    );
    this.costs.apify += cost;
    this.costs.total += cost;
  }
  
  trackAICall(model: string, tokensIn: number, tokensOut: number, duration: number) {
    const pricing = AI_PRICING[model];
    const cost = (tokensIn / 1_000_000) * pricing.per_1m_in +
                 (tokensOut / 1_000_000) * pricing.per_1m_out;
    
    this.costs.ai_calls.push({
      model,
      tokens_in: tokensIn,
      tokens_out: tokensOut,
      cost,
      duration_ms: duration
    });
    this.costs.total += cost;
  }
  
  getBreakdown() {
    return {
      apify_cost: this.costs.apify,
      ai_costs: this.costs.ai_calls,
      total_cost: this.costs.total,
      cost_by_provider: {
        apify: this.costs.apify,
        openai: this.costs.ai_calls
          .filter(c => c.model.includes('gpt'))
          .reduce((sum, c) => sum + c.cost, 0),
        claude: this.costs.ai_calls
          .filter(c => c.model.includes('claude'))
          .reduce((sum, c) => sum + c.cost, 0)
      },
      margin: {
        light: 0.97 - this.costs.total,    // 1 credit revenue
        deep: 4.85 - this.costs.total,     // 5 credits revenue
        xray: 5.82 - this.costs.total      // 6 credits revenue
      }
    };
  }
}
```

---

### **7. STEP-BY-STEP PERFORMANCE TRACKING**

**Why:** Need to identify bottlenecks, not just total time.

```typescript
// features/analysis/domain/services/performance-tracker.service.ts

export class PerformanceTracker {
  private steps: PerformanceStep[] = [];
  
  startStep(name: string) {
    this.steps.push({
      name,
      start: Date.now(),
      end: null,
      duration_ms: null
    });
  }
  
  endStep(name: string) {
    const step = this.steps.find(s => s.name === name && !s.end);
    if (step) {
      step.end = Date.now();
      step.duration_ms = step.end - step.start;
    }
  }
  
  getBreakdown() {
    const total = this.steps.reduce((sum, s) => sum + (s.duration_ms || 0), 0);
    const bottleneck = this.steps.reduce((max, s) =>
      (s.duration_ms || 0) > (max.duration_ms || 0) ? s : max
    );
    
    return {
      steps: this.steps.map(s => ({
        step: s.name,
        duration_ms: s.duration_ms,
        percentage: ((s.duration_ms || 0) / total) * 100
      })),
      total_duration_ms: total,
      bottleneck: {
        step: bottleneck.name,
        duration_ms: bottleneck.duration_ms
      }
    };
  }
}

// Usage in analysis
const perf = new PerformanceTracker();

perf.startStep('cache_check');
const cached = await checkCache();
perf.endStep('cache_check');

perf.startStep('apify_scrape');
const profile = await scrapeProfile();
perf.endStep('apify_scrape');

perf.startStep('ai_analysis');
const analysis = await analyzeProfile();
perf.endStep('ai_analysis');

console.log(perf.getBreakdown());
// Output:
// {
//   steps: [
//     { step: 'cache_check', duration_ms: 50, percentage: 0.5% },
//     { step: 'apify_scrape', duration_ms: 7200, percentage: 72% },
//     { step: 'ai_analysis', duration_ms: 2750, percentage: 27.5% }
//   ],
//   bottleneck: { step: 'apify_scrape', duration_ms: 7200 }
// }
```

---

### **8. CENTRALIZED CONFIGURATION**

**Single source of truth for all values:**

```typescript
// features/analysis/domain/config/analysis.config.ts

export const ANALYSIS_CONFIG = {
  LIGHT: {
    internal_name: 'light',
    display_name: 'Quick Analysis',
    db_value: 'light',
    credit_cost: 1,
    price_usd: 0.97,
    
    performance: {
      target_duration_ms: 6000,
      acceptable_max_ms: 11000,
      cache_ttl_hours: 24
    },
    
    ai: {
      model: 'gpt-4o-mini',
      reasoning_effort: 'low',
      max_tokens: 1000,
      use_prompt_cache: true
    },
    
    scraping: {
      required: true,
      posts_limit: 12,
      use_cache: true
    },
    
    output: {
      includes: ['score', 'brief_summary', 'confidence'],
      excludes: ['deep_insights', 'outreach']
    }
  },
  
  DEEP: {
    internal_name: 'deep',
    display_name: 'Profile Analysis',
    db_value: 'deep',
    credit_cost: 5,
    price_usd: 4.85,
    
    performance: {
      target_duration_ms: 20000,
      acceptable_max_ms: 25000,
      cache_ttl_hours: 12
    },
    
    ai: {
      parallel_calls: 3,
      models: {
        core_strategy: 'gpt-5-mini',
        outreach: 'gpt-5-mini',
        personality: 'gpt-5-mini'
      },
      use_prompt_cache: true,
      use_batch_api: false  // Add later
    },
    
    scraping: {
      required: true,
      posts_limit: 50,
      use_cache: true
    },
    
    output: {
      includes: ['score', 'deep_summary', 'outreach', 'personality', 'selling_points'],
      excludes: ['psychographic_deep_dive']
    }
  },
  
  XRAY: {
    internal_name: 'xray',
    display_name: 'X-Ray Analysis',
    db_value: 'xray',
    credit_cost: 6,
    price_usd: 5.82,
    
    performance: {
      target_duration_ms: 18000,
      acceptable_max_ms: 25000,
      cache_ttl_hours: 6
    },
    
    ai: {
      parallel_calls: 4,
      models: {
        psychographic: 'gpt-5',
        commercial: 'gpt-5-mini',
        outreach: 'gpt-5-mini',
        personality: 'gpt-5-mini'
      },
      use_prompt_cache: true,
      use_semantic_cache: false  // Add later
    },
    
    scraping: {
      required: true,
      posts_limit: 50,
      use_cache: true
    },
    
    output: {
      includes: ['all'],
      excludes: []
    }
  }
} as const;

// Type-safe accessor
export function getAnalysisConfig(type: 'light' | 'deep' | 'xray') {
  return ANALYSIS_CONFIG[type.toUpperCase()];
}
```

---

### **9. BULK ANALYSIS BATCHING (Apify-Aware)**

**Handle Apify concurrency limits gracefully:**

```typescript
// features/analysis/application/use-cases/bulk-analyze.usecase.ts

export async function handleBulkAnalyze(
  usernames: string[],
  analysisType: AnalysisType,
  businessId: string,
  userId: string
) {
  const APIFY_CONCURRENCY = 10; // Starter plan limit
  
  // Create bulk job
  const job_id = generateJobId();
  await db.insert('bulk_jobs', {
    job_id,
    user_id: userId,
    business_id: businessId,
    total_count: usernames.length,
    completed_count: 0,
    status: 'queued'
  });
  
  // Split into batches
  const batches = chunkArray(usernames, APIFY_CONCURRENCY);
  
  // Queue all batches
  for (let i = 0; i < batches.length; i++) {
    await queue.send({
      type: 'bulk_batch',
      job_id,
      batch_number: i + 1,
      total_batches: batches.length,
      usernames: batches[i],
      analysisType,
      businessId,
      userId
    });
  }
  
  return {
    job_id,
    total: usernames.length,
    batches: batches.length,
    estimated_time: `${batches.length * 10}s`,
    message: `Processing ${usernames.length} profiles in ${batches.length} batches`
  };
}

// Progress endpoint
export async function getBulkJobStatus(job_id: string) {
  const job = await db.query('SELECT * FROM bulk_jobs WHERE job_id = $1', [job_id]);
  return {
    job_id,
    status: job.status,
    completed: job.completed_count,
    total: job.total_count,
    progress: Math.round((job.completed_count / job.total_count) * 100),
    results_url: `/bulk-jobs/${job_id}/results`
  };
}
```

**User Experience:**
```
POST /v1/bulk-analyze { usernames: [100 profiles] }
  ‚Üì
Response:
{
  job_id: 'bulk_xyz',
  total: 100,
  batches: 10,
  estimated_time: '100s',
  poll_url: '/bulk-jobs/bulk_xyz'
}

// Poll every 3s
GET /bulk-jobs/bulk_xyz
{
  status: 'processing',
  completed: 47,
  total: 100,
  progress: 47%,
  eta: '53s'
}
```

---

## **üéØ PHASE 3: FUTURE ENHANCEMENTS (Month 2+)**

### **10. Cloudflare Analytics Engine**

Replace manual logging with native analytics:

```typescript
// Track every request
env.ANALYTICS_ENGINE.writeDataPoint({
  blobs: [run_id, username, analysisType],
  doubles: [cost, duration_ms, score],
  indexes: [userId, businessId]
});

// Query later
SELECT
  blob1 as analysis_type,
  AVG(double2) as avg_duration_ms,
  SUM(double1) as total_cost,
  COUNT(*) as total_runs
FROM analytics
WHERE index1 = 'user_123'
AND timestamp > NOW() - INTERVAL '7 days'
GROUP BY analysis_type;
```

---

### **11. Semantic Caching (XRAY only)**

Cache similar queries using embeddings:

```typescript
// Check if similar analysis exists
const embedding = await getEmbedding(`${business_context}|${profile.bio}`);
const similar = await vectorDB.search(embedding, threshold: 0.9);
if (similar) {
  return adaptCachedResult(similar, profile);
}
```

---

### **12. Batch API (Cost Optimization)**

Combine multiple AI calls into single batch request (50% discount):

```typescript
// Instead of 3 separate calls
const batch = await openai.batches.create({
  requests: [
    { id: 'core', messages: coreMessages },
    { id: 'outreach', messages: outreachMessages },
    { id: 'personality', messages: personalityMessages }
  ]
});
```

---

## **üìä EXPECTED IMPROVEMENTS SUMMARY**

| Metric | Before | After Phase 1 | After Phase 2 | After Phase 3 |
|--------|--------|---------------|---------------|---------------|
| **LIGHT (cold)** | 10-13s | 8-11s | 7-10s | 6-9s |
| **LIGHT (cached)** | N/A | 2-3s | 1-2s | <1s |
| **DEEP (cold)** | 30s | 19-23s | 18-22s | 16-20s |
| **DEEP (cached)** | N/A | 12-15s | 11-14s | 10-12s |
| **XRAY (cold)** | 35s | 18-22s | 16-20s | 14-18s |
| **XRAY (cached)** | N/A | 10-12s | 9-11s | 8-10s |
| **Cost per LIGHT** | $0.0024 | $0.0005 | $0.0003 | $0.0002 |
| **Bulk 100 (LIGHT)** | 16min | 80-120s | 70-100s | 60-90s |
| **Cache hit rate** | 0% | 30% | 50% | 60% |

---

## **‚úÖ MUST-ADDS CHECKLIST**

### **Week 1 (Critical Path):**
- [ ] Global R2 caching (all profiles)
- [ ] Prompt caching (business context)
- [ ] Full AI parallelization (DEEP/XRAY)
- [ ] Async execution (Queue + Workers)

### **Week 2 (Performance):**
- [ ] Intelligent model selection
- [ ] Cost tracker (Apify + AI)
- [ ] Performance tracker (step-by-step)
- [ ] Centralized config file

### **Week 3 (Bulk):**
- [ ] Apify-aware batching
- [ ] Bulk job progress tracking
- [ ] Real-time status updates

### **Week 4 (Polish):**
- [ ] Error handling improvements
- [ ] Retry logic cleanup
- [ ] Admin dashboard updates
- [ ] Documentation

---

**This is your complete battle plan. Ready to execute?**


1. Path Alias Runtime Resolution
‚úÖ CONFIRMED: Wrangler 3+ handles automatically
How it works:

Wrangler internally uses esbuild
esbuild reads tsconfig.json paths configuration
Transforms @/api/* ‚Üí src/api/* at build time
No custom build config needed

toml# wrangler.toml - No [build] section required
[build.upload]
format = "modules"
main = "./src/index.ts"
Your existing tsconfig.json paths work out-of-box.

2. AWS Secrets - Cold Start Behavior
‚úÖ CONFIRMED: Cache persists across requests in same Worker instance
Current behavior:

First request (cold start): ~200-500ms penalty fetching from AWS
Subsequent requests: <1ms (cache hit)
Cache duration: 5 minutes per secret
Cache scope: Per Worker instance (not shared across instances)

typescriptconst secretsCache = new Map<string, { value: string; cachedAt: number }>();
const CACHE_TTL = 5 * 60 * 1000; // 5 minutes
NOT hitting AWS on every request ‚úÖ

3. RLS Performance Fix Pattern
‚úÖ CONFIRMED: Wrap auth.uid() in subquery
sql-- ‚ùå SLOW (runs auth.uid() per row scanned)
WHERE user_id = auth.uid()

-- ‚úÖ FAST (runs auth.uid() once, uses value)
WHERE user_id = (SELECT auth.uid())
```

**Apply to ALL policies in these tables:**
- leads
- runs  
- payloads
- business_profiles
- credit_balances
- credit_transactions

**Impact:** 100x faster (Supabase-validated claim)

---

## **4. Feature-First vs Current Structure**
‚úÖ **CONFIRMED:** New repository, copy/split working code

**Approach:**
- Create fresh repo with new structure
- Copy working code from current repo
- Split into vertical feature slices
- Remake implementations where needed
- **NOT refactoring in-place**

**Migration strategy:**
```
Old: src/api/credits/balance.controller.ts
New: features/credits/controllers/balance.controller.ts

Old: src/domain/ai/prompts.ts  
New: features/analysis/domain/services/ai-prompts.service.ts

5. Workflows Bindings & Dependencies
‚úÖ CONFIRMED: Workflows receive env object automatically
How Workflows access everything:
typescriptexport class DeepAnalysisWorkflow extends WorkflowEntrypoint<Env, Params> {
  async run(event: WorkflowEvent<Params>, step: WorkflowStep) {
    // ‚úÖ this.env contains ALL bindings:
    // - KV namespaces
    // - R2 buckets
    // - AWS credentials
    // - All env vars
    
    const profile = await step.do('scrape', async () => {
      // Access AWS secrets
      const apifyKey = await getApiKey('APIFY_API_TOKEN', this.env, this.env.APP_ENV);
      
      // Access R2
      const cached = await this.env.R2_CACHE_BUCKET.get(key);
      
      // Access KV
      const rateLimitData = await this.env.OSLIRA_KV.get(key);
      
      return await scrapeProfile(username, apifyKey);
    });
  }
}
Bindings configuration:
toml# wrangler.toml
[[workflows]]
binding = "ANALYSIS_WORKFLOW"
name = "deep-analysis-workflow"

# Workflows automatically inherit:
[[kv_namespaces]]
binding = "OSLIRA_KV"

[[r2_buckets]]  
binding = "R2_CACHE_BUCKET"

[vars]
AWS_ACCESS_KEY_ID = "..."
AWS_SECRET_ACCESS_KEY = "..."
AWS_REGION = "us-east-1"
APP_ENV = "production"
Each workflow step can:

Call getApiKey() with caching
Access KV/R2 directly via this.env
Use Supabase with credentials from AWS Secrets
Retry independently on infrastructure failures


6. Timeline Reconciliation
‚úÖ CONFIRMED: Merged into single phased approach
Correct implementation order:
Phase 0 (Week 1): Foundation

Validate all Cloudflare bindings (KV, R2, Workflows, Queues)
Apply RLS performance fix (wrap all auth.uid() calls)
Verify AWS secrets caching works
Health check endpoints with binding status

Phase 1 (Weeks 2-3): Critical Optimizations

R2 global profile caching
Prompt caching (business context)
Full AI parallelization (DEEP/XRAY)
Queue-based async execution
Intelligent model selection

Phase 2 (Weeks 4-5): Feature-First Refactor

New repository setup
Copy/split working code into vertical slices
Dual Supabase client pattern
Repository layer with interfaces

Phase 3 (Weeks 6-7): Async Infrastructure

Workflows for long-running analyses (20min+ jobs)
Queues for fire-and-forget (Stripe webhooks)
Progress tracking (SSE or Durable Objects)
AI Gateway integration

Phase 4 (Weeks 8-9): Observability

Analytics Engine (log every request)
Sentry (critical errors only)
Error classification (INFO/WARN/ERROR/CRITICAL)
Retry strategies (Critical/Standard/FastFail)

Phase 5 (Week 10): Testing

Vitest with @cloudflare/vitest-pool-workers
Domain layer: 100% coverage
Application layer: 90% coverage
Integration tests for key flows


üìä PERFORMANCE TARGETS - VALIDATED AS REALISTIC
Analysis TypeCurrentTarget (Phase 1)Target (Phase 3)LIGHT (cold)10-13s8-11s6-9sLIGHT (cached)N/A2-3s<1sDEEP (cold)30s19-23s16-20sDEEP (cached)N/A12-15s10-12sXRAY (cold)35s18-22s14-18sXRAY (cached)N/A10-12s8-10s
Key constraint acknowledged: Apify 6-8s minimum (unavoidable bottleneck)

üéØ CRITICAL OPTIMIZATIONS - CONFIRMED VIABLE
1. R2 Global Profile Caching
‚úÖ Impact: 50%+ cache hit rate after Month 1

Cache hits: <1s (vs 8s scraping)
Cost savings: $0.01 per cached request
TTL: 24h (light), 12h (deep), 6h (xray)

2. Prompt Caching (Business Context)
‚úÖ Impact: 90% cost reduction on cached tokens

Business context (800 tokens): $0.003 ‚Üí $0.0003
Speed: 2-4s faster per analysis
Works for both OpenAI (auto) and Claude (cache_control)

3. Full AI Parallelization
‚úÖ Impact:

DEEP: 19s ‚Üí 15s (4s saved)
XRAY: 22s ‚Üí 10s (12s saved)

typescript// BEFORE (sequential)
const core = await executeCoreStrategy();      // 15s
const outreach = await executeOutreach();      // 4s
// Total: 19s

// AFTER (parallel)
const [core, outreach, personality] = await Promise.all([
  executeCoreStrategy(),   // 15s
  executeOutreach(),       // 4s  
  executePersonality()     // 4s
]);
// Total: 15s (saved 4s)
4. Queue-Based Async Execution
‚úÖ Impact: No 30s timeout, better UX
typescriptPOST /v1/analyze
  ‚Üì (400ms response)
{ run_id, status: 'queued', poll_url: '/runs/run_id' }

// Frontend polls every 2s
GET /runs/run_id
{ status: 'analyzing', progress: 'Scraping profile...', elapsed: 8 }
5. AI Gateway Integration
‚úÖ Impact: FREE cost savings + fallback

30-40% cost reduction (caching duplicate prompts)
Automatic fallback (OpenAI down ‚Üí Claude)
Free analytics (track costs per user)
One-line change to baseURL

‚úÖ CONFIRMED: Durable Objects justified
Valid use cases:

Accurate progress tracking - Workflow sends granular updates to DO, client polls DO state
Cancel capability - Client sends cancel signal ‚Üí DO ‚Üí Workflow abort


üéØ DURABLE OBJECT PATTERN FOR ANALYSIS PROGRESS
typescript// core/durable-objects/analysis-progress.do.ts

export class AnalysisProgressTracker extends DurableObject {
  private state: {
    run_id: string;
    status: 'queued' | 'scraping' | 'analyzing' | 'saving' | 'complete' | 'failed' | 'cancelled';
    progress: number; // 0-100
    current_step: string;
    elapsed_ms: number;
    should_cancel: boolean;
  };

  constructor(state: DurableObjectState, env: Env) {
    super(state, env);
  }

  async fetch(request: Request) {
    const url = new URL(request.url);
    
    // GET /progress - Client polls this
    if (request.method === 'GET' && url.pathname === '/progress') {
      return Response.json({
        run_id: this.state.run_id,
        status: this.state.status,
        progress: this.state.progress,
        current_step: this.state.current_step,
        elapsed_ms: this.state.elapsed_ms
      });
    }
    
    // POST /update - Workflow sends updates here
    if (request.method === 'POST' && url.pathname === '/update') {
      const update = await request.json();
      this.state = { ...this.state, ...update };
      await this.ctx.storage.put('state', this.state);
      return Response.json({ ok: true });
    }
    
    // POST /cancel - Client cancels analysis
    if (request.method === 'POST' && url.pathname === '/cancel') {
      this.state.should_cancel = true;
      this.state.status = 'cancelled';
      await this.ctx.storage.put('state', this.state);
      return Response.json({ ok: true, message: 'Cancel signal sent' });
    }
    
    // GET /should-cancel - Workflow checks this between steps
    if (request.method === 'GET' && url.pathname === '/should-cancel') {
      return Response.json({ should_cancel: this.state.should_cancel });
    }
    
    return new Response('Not Found', { status: 404 });
  }
}

WORKFLOW INTEGRATION
typescript// core/workflows/deep-analysis.workflow.ts

export class DeepAnalysisWorkflow extends WorkflowEntrypoint<Env, AnalysisParams> {
  async run(event: WorkflowEvent<AnalysisParams>, step: WorkflowStep) {
    const { run_id, username, business_id, user_id } = event.payload;
    
    // Get DO stub
    const doId = this.env.ANALYSIS_PROGRESS.idFromName(run_id);
    const progressTracker = this.env.ANALYSIS_PROGRESS.get(doId);
    
    // Helper: Update progress
    const updateProgress = async (status: string, progress: number, current_step: string) => {
      await progressTracker.fetch('https://fake-host/update', {
        method: 'POST',
        body: JSON.stringify({ status, progress, current_step })
      });
    };
    
    // Helper: Check if cancelled
    const shouldCancel = async (): Promise<boolean> => {
      const response = await progressTracker.fetch('https://fake-host/should-cancel');
      const data = await response.json();
      return data.should_cancel;
    };
    
    try {
      // Initialize
      await updateProgress('queued', 0, 'Starting analysis');
      
      // Step 1: Check cache
      if (await shouldCancel()) throw new Error('CANCELLED');
      await updateProgress('scraping', 10, 'Checking cache');
      
      const cached = await step.do('check_cache', async () => {
        return await this.env.R2_CACHE_BUCKET.get(`instagram:${username}:v1`);
      });
      
      if (cached) {
        await updateProgress('complete', 100, 'Loaded from cache');
        return await completeFromCache(run_id, cached);
      }
      
      // Step 2: Scrape profile
      if (await shouldCancel()) throw new Error('CANCELLED');
      await updateProgress('scraping', 25, 'Scraping Instagram profile');
      
      const profile = await step.do('scrape_profile', async () => {
        const apifyKey = await getApiKey('APIFY_API_TOKEN', this.env, this.env.APP_ENV);
        return await scrapeInstagramProfile(username, apifyKey);
      });
      
      // Step 3: Run AI analysis (parallel)
      if (await shouldCancel()) throw new Error('CANCELLED');
      await updateProgress('analyzing', 50, 'Running AI analysis (3 parallel calls)');
      
      const [coreAnalysis, outreach, personality] = await step.do('ai_analysis', async () => {
        const business = await fetchBusinessProfile(business_id, user_id, this.env);
        
        return await Promise.all([
          executeDeepAnalysis(profile, business),     // 15s
          generateOutreachMessage(profile, business), // 4s
          analyzePersonality(profile)                 // 4s
        ]);
      });
      
      // Step 4: Save results
      if (await shouldCancel()) throw new Error('CANCELLED');
      await updateProgress('saving', 90, 'Saving analysis results');
      
      await step.do('save_results', async () => {
        const analysisData = {
          core: coreAnalysis,
          outreach,
          personality
        };
        
        await saveCompleteAnalysis(run_id, analysisData, this.env);
      });
      
      // Complete
      await updateProgress('complete', 100, 'Analysis complete');
      
    } catch (error: any) {
      if (error.message === 'CANCELLED') {
        await updateProgress('cancelled', 0, 'Analysis cancelled by user');
      } else {
        await updateProgress('failed', 0, `Error: ${error.message}`);
      }
      throw error;
    }
  }
}

CLIENT-SIDE INTEGRATION
typescript// Frontend: Start analysis
const response = await fetch('/v1/analyze', {
  method: 'POST',
  body: JSON.stringify({
    username: 'nike',
    analysis_type: 'deep',
    business_id: 'biz_123'
  })
});

const { run_id } = await response.json();
// { run_id: 'run_abc123', status: 'queued' }

// Poll progress every 2 seconds
const interval = setInterval(async () => {
  const progress = await fetch(`/runs/${run_id}/progress`);
  const data = await progress.json();
  
  console.log(`${data.status}: ${data.progress}% - ${data.current_step}`);
  
  if (data.status === 'complete' || data.status === 'failed' || data.status === 'cancelled') {
    clearInterval(interval);
  }
}, 2000);

// Cancel button handler
cancelButton.onclick = async () => {
  await fetch(`/runs/${run_id}/cancel`, { method: 'POST' });
  console.log('Cancel signal sent');
};

WORKER ENDPOINT FOR PROGRESS
typescript// src/index.ts

app.get('/runs/:run_id/progress', async (c) => {
  const run_id = c.req.param('run_id');
  
  // Get DO stub
  const doId = c.env.ANALYSIS_PROGRESS.idFromName(run_id);
  const progressTracker = c.env.ANALYSIS_PROGRESS.get(doId);
  
  // Forward request to DO
  const response = await progressTracker.fetch('https://fake-host/progress');
  return response;
});

app.post('/runs/:run_id/cancel', async (c) => {
  const run_id = c.req.param('run_id');
  
  // Get DO stub
  const doId = c.env.ANALYSIS_PROGRESS.idFromName(run_id);
  const progressTracker = c.env.ANALYSIS_PROGRESS.get(doId);
  
  // Send cancel signal
  const response = await progressTracker.fetch('https://fake-host/cancel', {
    method: 'POST'
  });
  
  return c.json({ success: true, message: 'Analysis cancellation requested' });
});

WRANGLER.TOML CONFIG
toml[durable_objects]
bindings = [
  { name = "ANALYSIS_PROGRESS", class_name = "AnalysisProgressTracker", script_name = "oslira-workers" }
]

[[migrations]]
tag = "v1"
new_classes = ["AnalysisProgressTracker"]

BENEFITS OF THIS PATTERN
‚úÖ Accurate progress tracking

Workflow updates DO at each step
Client polls DO state (not database)
Real-time granular updates

‚úÖ Graceful cancellation

Client sends cancel signal ‚Üí DO sets flag
Workflow checks flag between steps
Stops processing, doesn't waste credits/API calls

‚úÖ State persistence

DO storage survives Worker restarts
Progress survives page refresh
Can resume UI from any point

‚úÖ No database spam

Don't write progress updates to Supabase
Only write final result when complete
DO is ephemeral cache (perfect for this)


COST ANALYSIS
Durable Objects pricing:

$0.15 per million requests
$0.20 per GB-month storage (you'll use <1MB per analysis)

Typical analysis:

1 DO create (start)
10-20 progress updates (workflow ‚Üí DO)
10-15 progress polls (client ‚Üí DO)
1 cancel check per step (workflow ‚Üí DO)
Total: ~30-50 DO requests = $0.0000075 per analysis

Negligible cost, massive UX improvement.

‚úÖ DURABLE OBJECT DECISION: JUSTIFIED AND VALIDATED
Add this to your implementation doc. Week 6-7 when you build Workflows infrastructure.

